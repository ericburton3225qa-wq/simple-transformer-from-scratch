# simple-transformer-from-scratch
This repository contains a fully custom implementation of a Transformer-based autoregressive language model inspired by GPT-2. The goal of this project is to provide an understandable, modular, and educational codebase that demonstrates how modern large language models are built from the ground up.
